{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.layers import Input, Concatenate, Dropout\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def contrast_stretching(img):\n",
    "    min_val = np.min(img)\n",
    "    max_val = np.max(img)\n",
    "    stretched_img = ((img - min_val) / (max_val - min_val)) * 255\n",
    "    return stretched_img.astype(np.uint8)\n",
    "\n",
    "\n",
    "def load_images_and_labels(parent_folder, is_training=True):\n",
    "    images = []\n",
    "    labels = []\n",
    "    cloud_distributions = []\n",
    "\n",
    "    images_folder = os.path.join(parent_folder, 'images')\n",
    "\n",
    "    if is_training:\n",
    "        labels_folder = os.path.join(parent_folder, 'labels')\n",
    "        cloud_distributions_folder = os.path.join(parent_folder, 'cloud_distributions')\n",
    "    else:\n",
    "        cloud_distributions_folder = os.path.join(parent_folder, 'cloud_distributions')\n",
    "\n",
    "    for img_name in os.listdir(images_folder):\n",
    "        img_path = os.path.join(images_folder, img_name)\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        img_stretched = contrast_stretching(img)\n",
    "\n",
    "        _, thresh = cv2.threshold(img_stretched, 128, 255, cv2.THRESH_OTSU)\n",
    "\n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        erosion = cv2.erode(thresh, kernel, iterations=1)\n",
    "        dilation = cv2.dilate(erosion, kernel, iterations=1)\n",
    "\n",
    "        img_resized = cv2.resize(dilation, (128, 128))\n",
    "        img_normalized = img_resized / 255.0\n",
    "\n",
    "        images.append(img_normalized)\n",
    "\n",
    "        if is_training:\n",
    "            # Load label (solar irradiance) from corresponding .txt file\n",
    "            label_file_path = os.path.join(labels_folder, img_name.replace('.png', '.txt'))\n",
    "            with open(label_file_path, 'r') as label_file:\n",
    "                label = float(label_file.read().strip())\n",
    "            labels.append(label)\n",
    "\n",
    "        # Load cloud distribution from corresponding .txt file\n",
    "        cloud_distribution_file_path = os.path.join(cloud_distributions_folder, img_name.replace('.png', '.txt'))\n",
    "        with open(cloud_distribution_file_path, 'r') as cloud_file:\n",
    "            cloud_distribution = float(cloud_file.read().strip())\n",
    "        cloud_distributions.append(cloud_distribution)\n",
    "\n",
    "    # Convert lists to NumPy arrays\n",
    "    images_array = np.array(images)\n",
    "    labels_array = np.array(labels) if is_training else None\n",
    "    cloud_distributions_array = np.array(cloud_distributions)\n",
    "\n",
    "    # Reshape images array to include a new axis\n",
    "    images_array = images_array[:, :, :, np.newaxis]\n",
    "\n",
    "    return images_array, labels_array, cloud_distributions_array\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\janrh\\\\OneDrive - University of the Philippines\\\\Personal Files\\\\Girry Thesis\\\\msu-gsc\\\\ee199-thesis\\\\training\\\\images'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m parent_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mjanrh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive - University of the Philippines\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPersonal Files\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGirry Thesis\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmsu-gsc\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mee199-thesis\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load training data\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m X_train, y_train, cloud_distribution_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_and_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparent_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load validation data\u001b[39;00m\n\u001b[0;32m      7\u001b[0m X_val, y_val, cloud_distribution_val \u001b[38;5;241m=\u001b[39m load_images_and_labels(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m), is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m, in \u001b[0;36mload_images_and_labels\u001b[1;34m(parent_folder, is_training)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     cloud_distributions_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_folder, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcloud_distributions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     22\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(images_folder, img_name)\n\u001b[0;32m     23\u001b[0m     img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(img_path, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\janrh\\\\OneDrive - University of the Philippines\\\\Personal Files\\\\Girry Thesis\\\\msu-gsc\\\\ee199-thesis\\\\training\\\\images'"
     ]
    }
   ],
   "source": [
    "\n",
    "parent_folder = r'C:\\Users\\janrh\\OneDrive - University of the Philippines\\Personal Files\\Girry Thesis\\msu-gsc\\ee199-thesis'\n",
    "\n",
    "# Load training data\n",
    "X_train, y_train, cloud_distribution_train = load_images_and_labels(os.path.join(parent_folder, 'training'))\n",
    "\n",
    "# Load validation data\n",
    "X_val, y_val, cloud_distribution_val = load_images_and_labels(os.path.join(parent_folder, 'validation'), is_training=False)\n",
    "\n",
    "\n",
    "\n",
    "# Model Creation\n",
    "def create_model(input_shape, num_features):\n",
    "    input_img = Input(shape=input_shape, name='image_input')\n",
    "    input_features = Input(shape=(num_features,), name='cloud_distribution_input')\n",
    "    X = Conv2D(64, (3, 3), activation='relu')(input_img)\n",
    "    X = MaxPooling2D((2, 2))(X)\n",
    "    X = Conv2D(128, (3, 3), activation='relu')(X)\n",
    "    X = MaxPooling2D((2, 2))(X)\n",
    "    X = Conv2D(256, (3, 3), activation='relu')(X)\n",
    "    X = MaxPooling2D((2, 2))(X)\n",
    "    X = Flatten()(X)\n",
    "\n",
    "    merged = Concatenate()([X, input_features])\n",
    "\n",
    "    X = Dense(256, activation='relu')(merged)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(128, activation='tanh')(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "\n",
    "    output = Dense(1, activation=\"linear\")(X)\n",
    "\n",
    "    model = Model(inputs=[input_img, input_features], outputs=output)\n",
    "\n",
    "    return model\n",
    "\n",
    "#Additional Metrics\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return tf.sqrt(tf.keras.losses.MeanSquaredError()(y_true, y_pred))\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = tf.convert_to_tensor(y_true), tf.convert_to_tensor(y_pred)\n",
    "    return tf.reduce_mean(tf.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "model = create_model(input_shape=(128, 128, 1), num_features=1)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='mean_squared_error',\n",
    "    metrics=[root_mean_squared_error, 'mean_absolute_error', mean_absolute_percentage_error],\n",
    ")\n",
    "\n",
    "# Custom Data Generator\n",
    "def custom_generator(X_img, cloud_distribution, y, batch_size):\n",
    "    image_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=90)\n",
    "    while True:\n",
    "        idx = np.arange(len(X_img))\n",
    "        np.random.shuffle(idx)\n",
    "        for i in range(0, len(X_img), batch_size):\n",
    "            # Training\n",
    "            batch_idx = idx[i:i+batch_size]\n",
    "            X_img_batch = X_img[batch_idx]\n",
    "            cloud_distribution_batch = cloud_distribution[batch_idx]\n",
    "            y_batch = y[batch_idx]\n",
    "            # Perform image augmentation here\n",
    "            X_img_batch = image_gen.flow(X_img_batch, batch_size=batch_size, shuffle=False).next()\n",
    "            yield [X_img_batch, cloud_distribution_batch], y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "\n",
    "batch_size = 64\n",
    "train_gen = custom_generator(X_train, cloud_distribution_train, y_train, batch_size)\n",
    "steps_per_epoch = np.ceil(len(X_train) / batch_size)\n",
    "\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=([X_val, cloud_distribution_val], y_val),\n",
    "    epochs=200,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Define the file path where you want to save the model\n",
    "model_save_path = 'solar_irradiance_model.h5'\n",
    "\n",
    "# Save the model\n",
    "model.save(model_save_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
